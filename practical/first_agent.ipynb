{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "```shell\n",
    "sudo apt-get update\n",
    "sudo apt-get install g++\n",
    "sudo apt install swig cmake\n",
    "sudo apt-get install x11-utils\n",
    "sudo apt-get install -y python3-opengl\n",
    "sudo apt-get install -y ffmpeg\n",
    "sudo apt-get install -y xvfb\n",
    "pip3 install pyvirtualdisplay\n",
    "# finally run\n",
    "pip install -r requirements.txt\n",
    "# to use jupyter notebook in VScode\n",
    "pip install ipykernel\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f797ee24970>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Virtual display\n",
    "import os\n",
    "os.environ['PYVIRTUALDISPLAY_DISPLAYFD'] = '0'\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/naveen/miniconda3/envs/rl_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# for environment rendering\n",
    "import gymnasium\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# for deep RL Library\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Gymnasium and how it works ü§ñ\n",
    "\n",
    "üèã The library containing our environment is called Gymnasium.\n",
    "**We'll use Gymnasium a lot in Deep Reinforcement Learning.**\n",
    "\n",
    "Gymnasium is the **new version of Gym library** [maintained by the Farama Foundation](https://farama.org/).\n",
    "\n",
    "The Gymnasium library provides two things:\n",
    "\n",
    "- An interface that allows you to **create RL environments**.\n",
    "- A **collection of environments** (gym-control, atari, box2D...).\n",
    "\n",
    "Let's look at an example, but first let's recall the RL loop.\n",
    "\n",
    "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"The RL process\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step:\n",
    "- Our Agent receives¬†a **state (S0)**¬†from the¬†**Environment**¬†‚Äî we receive the first frame of our game (Environment).\n",
    "- Based on that¬†**state (S0),**¬†the Agent takes an¬†**action (A0)**¬†‚Äî our Agent will move to the right.\n",
    "- The environment transitions to a¬†**new**¬†**state (S1)**¬†‚Äî new frame.\n",
    "- The environment gives some¬†**reward (R1)**¬†to the Agent ‚Äî we‚Äôre not dead¬†*(Positive Reward +1)*.\n",
    "\n",
    "\n",
    "With Gymnasium:\n",
    "\n",
    "1Ô∏è‚É£ We create our environment using `gymnasium.make()`\n",
    "\n",
    "2Ô∏è‚É£ We reset the environment to its initial state with `observation = env.reset()`\n",
    "\n",
    "At each step:\n",
    "\n",
    "3Ô∏è‚É£ Get an action using our model (in our example we take a random action)\n",
    "\n",
    "4Ô∏è‚É£ Using `env.step(action)`, we perform this action in the environment and get\n",
    "- `observation`: The new state (st+1)\n",
    "- `reward`: The reward we get after executing the action\n",
    "- `terminated`: Indicates if the episode terminated (agent reach the terminal state)\n",
    "- `truncated`: Introduced with this new version, it indicates a timelimit or if an agent go out of bounds of the environment for instance.\n",
    "- `info`: A dictionary that provides additional information (depends on the environment).\n",
    "\n",
    "For more explanations check this üëâ https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
    "\n",
    "If the episode is terminated:\n",
    "- We reset the environment to its initial state with `observation = env.reset()`\n",
    "\n",
    "**Let's look at an example!** Make sure to read the code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00473328  1.4141573   0.47941065  0.14387114 -0.00547786 -0.10859375\n",
      "  0.          0.        ]\n",
      "Action taken: 1\n",
      "Action taken: 2\n",
      "Action taken: 0\n",
      "Action taken: 1\n",
      "Action taken: 0\n",
      "Action taken: 1\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 1\n",
      "Action taken: 2\n",
      "Action taken: 1\n",
      "Action taken: 2\n",
      "Action taken: 3\n",
      "Action taken: 3\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 1\n",
      "Action taken: 1\n",
      "Action taken: 2\n",
      "Action taken: 3\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# first, we create our environment called LunarLander-v2\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# Then we reset this environement,  initial state\n",
    "observation, info = env.reset()\n",
    "print(observation)\n",
    "for _ in range(20):\n",
    "    # Take a random action\n",
    "    action = env.action_space.sample()\n",
    "    print(\"Action taken:\", action)\n",
    "    # Do this action in the enivornment and get\n",
    "    # next_state, reward, terminated, truncated, info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
    "    if terminated or truncated:\n",
    "        # Reset the environment\n",
    "        print(\"Environment is reset\")\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the [Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/) looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Observation Space__\n",
      "(8,)\n",
      "Sample observation: [49.690548   34.674305    2.274109    3.27103     1.1788406  -2.7548833\n",
      "  0.05388867  0.35002184]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"__Observation Space__\")\n",
    "print(env.observation_space.shape)\n",
    "print(\"Sample observation:\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see with `Observation Space Shape (8,)` that the observation is a vector of size 8, where each value contains different information about the lander:\n",
    "- Horizontal pad coordinate (x)\n",
    "- Vertical pad coordinate (y)\n",
    "- Horizontal speed (x)\n",
    "- Vertical speed (y)\n",
    "- Angle\n",
    "- Angular speed\n",
    "- If the left leg contact point has touched the land (boolean)\n",
    "- If the right leg contact point has touched the land (boolean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Action Space__\n",
      "Action Space Shape 4\n",
      "Action Space Sample: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"__Action Space__\")\n",
    "print(\"Action Space Shape\",env.action_space.n)\n",
    "print(\"Action Space Sample:\", env.action_space.sample()) # Get a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space (the set of possible actions the agent can take) is discrete with 4 actions available üéÆ:\n",
    "\n",
    "- Action 0: Do nothing,\n",
    "- Action 1: Fire left orientation engine,\n",
    "- Action 2: Fire the main engine,\n",
    "- Action 3: Fire right orientation engine.\n",
    "\n",
    "Reward function (the function that will give a reward at each timestep) üí∞:\n",
    "\n",
    "After every step a reward is granted. The total reward of an episode is the **sum of the rewards for all the steps within that episode**.\n",
    "\n",
    "For each step, the reward:\n",
    "\n",
    "- Is increased/decreased the closer/further the lander is to the landing pad.\n",
    "-  Is increased/decreased the slower/faster the lander is moving.\n",
    "- Is decreased the more the lander is tilted (angle not horizontal).\n",
    "- Is increased by 10 points for each leg that is in contact with the ground.\n",
    "- Is decreased by 0.03 points each frame a side engine is firing.\n",
    "- Is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receive an **additional reward of -100 or +100 points for crashing or landing safely respectively.**\n",
    "\n",
    "An episode is **considered a solution if it scores at least 200 points.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Environment\n",
    "\n",
    "- We create a vectorized environment (a method for stacking multiple independent environments into a single environment) of 16 environments, this way, **we'll have more diverse experiences during the training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env('LunarLander-v2', n_envs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model ü§ñ\n",
    "- We have studied our environment and we understood the problem: **being able to land the Lunar Lander to the Landing Pad correctly by controlling left, right and main orientation engine**. Now let's build the algorithm we're going to use to solve this Problem üöÄ.\n",
    "\n",
    "- To do so, we're going to use our first Deep RL library, [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/).\n",
    "\n",
    "- SB3 is a set of **reliable implementations of reinforcement learning algorithms in PyTorch**.\n",
    "\n",
    "---\n",
    "\n",
    "üí° A good habit when using a new library is to dive first on the documentation: https://stable-baselines3.readthedocs.io/en/master/ and then try some tutorials.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, we're going to use SB3 **PPO**. [PPO (aka Proximal Policy Optimization) is one of the SOTA (state of the art) Deep Reinforcement Learning algorithms that you'll study during this course](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).\n",
    "\n",
    "PPO is a combination of:\n",
    "- *Value-based reinforcement learning method*: learning an action-value function that will tell us the **most valuable action to take given a state and action**.\n",
    "- *Policy-based reinforcement learning method*: learning a policy that will **give us a probability distribution over actions**.\n",
    "\n",
    "**Thus, an Actor Critic**\n",
    "\n",
    "Stable-Baselines3 is easy to set up:\n",
    "\n",
    "1Ô∏è‚É£ You **create your environment** (in our case it was done above)\n",
    "\n",
    "2Ô∏è‚É£ You define the **model you want to use and instantiate this model** `model = PPO(\"MlpPolicy\")`\n",
    "\n",
    "3Ô∏è‚É£ You **train the agent** with `model.learn` and define the number of training timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 64,\n",
    "    n_epochs = 4,\n",
    "    gamma = 0.999,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the PPO agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.2     |\n",
      "|    ep_rew_mean     | -188     |\n",
      "| time/              |          |\n",
      "|    fps             | 1389     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 87             |\n",
      "|    ep_rew_mean          | -156           |\n",
      "| time/                   |                |\n",
      "|    fps                  | 853            |\n",
      "|    iterations           | 2              |\n",
      "|    time_elapsed         | 38             |\n",
      "|    total_timesteps      | 32768          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.0060808985   |\n",
      "|    clip_fraction        | 0.0307         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.38          |\n",
      "|    explained_variance   | -0.00049340725 |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 2.1e+03        |\n",
      "|    n_updates            | 4              |\n",
      "|    policy_gradient_loss | -0.00509       |\n",
      "|    value_loss           | 5.25e+03       |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 89.4         |\n",
      "|    ep_rew_mean          | -122         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 782          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065715387 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | -0.006111026 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.01e+03     |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | -0.00558     |\n",
      "|    value_loss           | 2.68e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 97.4         |\n",
      "|    ep_rew_mean          | -147         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 741          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 88           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008632088  |\n",
      "|    clip_fraction        | 0.0607       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.0011262894 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 865          |\n",
      "|    n_updates            | 12           |\n",
      "|    policy_gradient_loss | -0.00448     |\n",
      "|    value_loss           | 1.1e+03      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 92.4         |\n",
      "|    ep_rew_mean          | -118         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 698          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 117          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008091982  |\n",
      "|    clip_fraction        | 0.0556       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | -0.023699999 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 382          |\n",
      "|    n_updates            | 16           |\n",
      "|    policy_gradient_loss | -0.00456     |\n",
      "|    value_loss           | 1.34e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 96.8          |\n",
      "|    ep_rew_mean          | -103          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 677           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 145           |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008704733   |\n",
      "|    clip_fraction        | 0.0979        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | -0.0014373064 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 264           |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00463      |\n",
      "|    value_loss           | 567           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 94.9          |\n",
      "|    ep_rew_mean          | -78.6         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 670           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 171           |\n",
      "|    total_timesteps      | 114688        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008932986   |\n",
      "|    clip_fraction        | 0.0658        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | -0.0037238598 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 382           |\n",
      "|    n_updates            | 24            |\n",
      "|    policy_gradient_loss | -0.00342      |\n",
      "|    value_loss           | 548           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 98.1         |\n",
      "|    ep_rew_mean          | -64.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 198          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075865686 |\n",
      "|    clip_fraction        | 0.0593       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.0016438365 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 221          |\n",
      "|    n_updates            | 28           |\n",
      "|    policy_gradient_loss | -0.00378     |\n",
      "|    value_loss           | 373          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 113          |\n",
      "|    ep_rew_mean          | -46.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 225          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.010988845  |\n",
      "|    clip_fraction        | 0.0683       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | -0.034330368 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 189          |\n",
      "|    n_updates            | 32           |\n",
      "|    policy_gradient_loss | -0.00435     |\n",
      "|    value_loss           | 409          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 111          |\n",
      "|    ep_rew_mean          | -43.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 651          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 251          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.010081412  |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.26        |\n",
      "|    explained_variance   | -0.008568287 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 272          |\n",
      "|    n_updates            | 36           |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    value_loss           | 436          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 108           |\n",
      "|    ep_rew_mean          | -30.1         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 643           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 280           |\n",
      "|    total_timesteps      | 180224        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0051105106  |\n",
      "|    clip_fraction        | 0.0393        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.23         |\n",
      "|    explained_variance   | -0.0055242777 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 178           |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00356      |\n",
      "|    value_loss           | 433           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 118           |\n",
      "|    ep_rew_mean          | -19.7         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 615           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 319           |\n",
      "|    total_timesteps      | 196608        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00891551    |\n",
      "|    clip_fraction        | 0.0451        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.19         |\n",
      "|    explained_variance   | -3.552437e-05 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 193           |\n",
      "|    n_updates            | 44            |\n",
      "|    policy_gradient_loss | -0.0045       |\n",
      "|    value_loss           | 488           |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 138            |\n",
      "|    ep_rew_mean          | -7.22          |\n",
      "| time/                   |                |\n",
      "|    fps                  | 606            |\n",
      "|    iterations           | 13             |\n",
      "|    time_elapsed         | 351            |\n",
      "|    total_timesteps      | 212992         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.007588773    |\n",
      "|    clip_fraction        | 0.0486         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.15          |\n",
      "|    explained_variance   | -1.7881393e-06 |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 342            |\n",
      "|    n_updates            | 48             |\n",
      "|    policy_gradient_loss | -0.00395       |\n",
      "|    value_loss           | 502            |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 150           |\n",
      "|    ep_rew_mean          | -2.86         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 593           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 386           |\n",
      "|    total_timesteps      | 229376        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.011291489   |\n",
      "|    clip_fraction        | 0.0751        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.12         |\n",
      "|    explained_variance   | -7.390976e-06 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 403           |\n",
      "|    n_updates            | 52            |\n",
      "|    policy_gradient_loss | -0.00353      |\n",
      "|    value_loss           | 615           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 236          |\n",
      "|    ep_rew_mean          | 2.01         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 582          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 421          |\n",
      "|    total_timesteps      | 245760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062064566 |\n",
      "|    clip_fraction        | 0.04         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.15        |\n",
      "|    explained_variance   | 8.827448e-05 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 332          |\n",
      "|    n_updates            | 56           |\n",
      "|    policy_gradient_loss | -0.000988    |\n",
      "|    value_loss           | 652          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 350          |\n",
      "|    ep_rew_mean          | 8.67         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 569          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 459          |\n",
      "|    total_timesteps      | 262144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055600214 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | 0.0010569096 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 129          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00238     |\n",
      "|    value_loss           | 459          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 425         |\n",
      "|    ep_rew_mean          | 8.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 558         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 499         |\n",
      "|    total_timesteps      | 278528      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008193992 |\n",
      "|    clip_fraction        | 0.0399      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.31123412  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 168         |\n",
      "|    n_updates            | 64          |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    value_loss           | 280         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 519          |\n",
      "|    ep_rew_mean          | 20.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 541          |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055849366 |\n",
      "|    clip_fraction        | 0.011        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0.5045445    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 274          |\n",
      "|    n_updates            | 68           |\n",
      "|    policy_gradient_loss | -0.00236     |\n",
      "|    value_loss           | 298          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 615          |\n",
      "|    ep_rew_mean          | 27.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 535          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 581          |\n",
      "|    total_timesteps      | 311296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049627116 |\n",
      "|    clip_fraction        | 0.0255       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.74381685   |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 90.7         |\n",
      "|    n_updates            | 72           |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    value_loss           | 165          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 662          |\n",
      "|    ep_rew_mean          | 29.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 523          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 625          |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076554827 |\n",
      "|    clip_fraction        | 0.0585       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.2         |\n",
      "|    explained_variance   | 0.83288354   |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.7         |\n",
      "|    n_updates            | 76           |\n",
      "|    policy_gradient_loss | -0.00276     |\n",
      "|    value_loss           | 116          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 717          |\n",
      "|    ep_rew_mean          | 28.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 510          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 673          |\n",
      "|    total_timesteps      | 344064       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055499068 |\n",
      "|    clip_fraction        | 0.0396       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.15        |\n",
      "|    explained_variance   | 0.8462887    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.2         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00259     |\n",
      "|    value_loss           | 135          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 766         |\n",
      "|    ep_rew_mean          | 29.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 486         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 740         |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005606536 |\n",
      "|    clip_fraction        | 0.0429      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.8211168   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 74.8        |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | -0.00364    |\n",
      "|    value_loss           | 154         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 794          |\n",
      "|    ep_rew_mean          | 32.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 470          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 800          |\n",
      "|    total_timesteps      | 376832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067048986 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.8812634    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.3         |\n",
      "|    n_updates            | 88           |\n",
      "|    policy_gradient_loss | -0.00204     |\n",
      "|    value_loss           | 99.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 798          |\n",
      "|    ep_rew_mean          | 35.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 456          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 861          |\n",
      "|    total_timesteps      | 393216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042196084 |\n",
      "|    clip_fraction        | 0.0296       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.89873505   |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.4         |\n",
      "|    n_updates            | 92           |\n",
      "|    policy_gradient_loss | -0.00178     |\n",
      "|    value_loss           | 83.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 819          |\n",
      "|    ep_rew_mean          | 45.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 442          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 926          |\n",
      "|    total_timesteps      | 409600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060093086 |\n",
      "|    clip_fraction        | 0.0407       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.90677595   |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.3         |\n",
      "|    n_updates            | 96           |\n",
      "|    policy_gradient_loss | -0.00243     |\n",
      "|    value_loss           | 88.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 856         |\n",
      "|    ep_rew_mean          | 62.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 431         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 987         |\n",
      "|    total_timesteps      | 425984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005628754 |\n",
      "|    clip_fraction        | 0.0544      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.9215762   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26.3        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00144    |\n",
      "|    value_loss           | 55.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 892         |\n",
      "|    ep_rew_mean          | 82          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 421         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 1050        |\n",
      "|    total_timesteps      | 442368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005356878 |\n",
      "|    clip_fraction        | 0.0356      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.94703645  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.3        |\n",
      "|    n_updates            | 104         |\n",
      "|    policy_gradient_loss | -0.00136    |\n",
      "|    value_loss           | 33.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 885         |\n",
      "|    ep_rew_mean          | 90.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 411         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 1115        |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006280504 |\n",
      "|    clip_fraction        | 0.0731      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.9314468   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 58.5        |\n",
      "|    n_updates            | 108         |\n",
      "|    policy_gradient_loss | -0.00249    |\n",
      "|    value_loss           | 48          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 905          |\n",
      "|    ep_rew_mean          | 99.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 402          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 1180         |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045088707 |\n",
      "|    clip_fraction        | 0.0443       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.9652424    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.73         |\n",
      "|    n_updates            | 112          |\n",
      "|    policy_gradient_loss | -0.000843    |\n",
      "|    value_loss           | 29           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 932         |\n",
      "|    ep_rew_mean          | 109         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 395         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 1242        |\n",
      "|    total_timesteps      | 491520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004387265 |\n",
      "|    clip_fraction        | 0.037       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.9679712   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.29        |\n",
      "|    n_updates            | 116         |\n",
      "|    policy_gradient_loss | -0.000273   |\n",
      "|    value_loss           | 26.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 933          |\n",
      "|    ep_rew_mean          | 112          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 390          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 1300         |\n",
      "|    total_timesteps      | 507904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048170686 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.95694965   |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.1         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    value_loss           | 35.2         |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train it for 500,000 timesteps\n",
    "model.learn(total_timesteps=500_000)\n",
    "# Save the model\n",
    "model_name = \"ppo-LunarLander-v2\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=197.64 +/- 17.94574970642794\n"
     ]
    }
   ],
   "source": [
    "eval_env = Monitor(gym.make(\"LunarLander-v2\", render_mode='rgb_array'))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish our trained model on the Hub üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `package_to_hub` **we evaluate, record a replay, generate a model card of our agent and push it to the hub**.\n",
    "\n",
    "This way:\n",
    "- We can **showcase our work** üî•\n",
    "- We can **visualize our agent playing** üëÄ\n",
    "- We can **share with the community an agent that others can use** üíæ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "The token `RL_token` has been saved to /home/naveen/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/naveen/.cache/huggingface/token\n",
      "Login successful.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "access_token = os.environ.get('HF_TOKEN')\n",
    "USER_NAME = os.environ.get('USER_NAME')\n",
    "!huggingface-cli login --token $access_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill the `package_to_hub` function:\n",
    "- `model`: our trained model.\n",
    "- `model_name`: the name of the trained model that we defined in `model_save`\n",
    "- `model_architecture`: the model architecture we used, in our case PPO\n",
    "- `env_id`: the name of the environment, in our case `LunarLander-v2`\n",
    "- `eval_env`: the evaluation environment defined in eval_env\n",
    "- `repo_id`: the name of the Hugging Face Hub Repository that will be created/updated `(repo_id = {username}/{repo_name})`\n",
    "\n",
    "üí° **A good name is {username}/{model_architecture}-{env_id}**\n",
    "\n",
    "- `commit_message`: message of the commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ This function will save, evaluate, generate a video of your agent,\n",
      "create a model card and push everything to the hub. It might take up to 1min.\n",
      "This is a work in progress: if you encounter a bug, please open an issue.\u001b[0m\n",
      "Saving video to /tmp/tmpjdm0ayzg/-step-0-to-step-1000.mp4\n",
      "MoviePy - Building video /tmp/tmpjdm0ayzg/-step-0-to-step-1000.mp4.\n",
      "MoviePy - Writing video /tmp/tmpjdm0ayzg/-step-0-to-step-1000.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready /tmp/tmpjdm0ayzg/-step-0-to-step-1000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/tmpjdm0ayzg/-step-0-to-step-1000.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf61.1.100\n",
      "  Duration: 00:00:20.00, start: 0.000000, bitrate: 73 kb/s\n",
      "  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p, 600x400, 68 kb/s, 50 fps, 50 tbr, 12800 tbn, 100 tbc (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc61.3.100 libx264\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x55e10fd6cf00] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x55e10fd6cf00] profile High, level 3.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x55e10fd6cf00] 264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/tmp/tmpkflx8wkg/replay.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2avc1mp41\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv420p(progressive), 600x400, q=2-31, 50 fps, 12800 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame= 1000 fps=331 q=-1.0 Lsize=     173kB time=00:00:19.94 bitrate=  71.2kbits/s speed=6.61x    \n",
      "video:161kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 7.781269%\n",
      "[libx264 @ 0x55e10fd6cf00] frame I:5     Avg QP: 8.96  size:  1851\n",
      "[libx264 @ 0x55e10fd6cf00] frame P:255   Avg QP:24.08  size:   246\n",
      "[libx264 @ 0x55e10fd6cf00] frame B:740   Avg QP:23.78  size:   124\n",
      "[libx264 @ 0x55e10fd6cf00] consecutive B-frames:  0.8%  0.8%  2.4% 96.0%\n",
      "[libx264 @ 0x55e10fd6cf00] mb I  I16..4: 82.5% 11.8%  5.7%\n",
      "[libx264 @ 0x55e10fd6cf00] mb P  I16..4:  0.3%  0.5%  0.2%  P16..4:  2.2%  0.4%  0.2%  0.0%  0.0%    skip:96.3%\n",
      "[libx264 @ 0x55e10fd6cf00] mb B  I16..4:  0.0%  0.0%  0.1%  B16..8:  3.0%  0.2%  0.0%  direct: 0.1%  skip:96.5%  L0:54.2% L1:45.1% BI: 0.7%\n",
      "[libx264 @ 0x55e10fd6cf00] 8x8 transform intra:24.3% inter:16.1%\n",
      "[libx264 @ 0x55e10fd6cf00] coded y,uvDC,uvAC intra: 7.2% 12.5% 11.5% inter: 0.2% 0.3% 0.2%\n",
      "[libx264 @ 0x55e10fd6cf00] i16 v,h,dc,p: 82% 14%  5%  0%\n",
      "[libx264 @ 0x55e10fd6cf00] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu:  7% 18% 74%  0%  0%  0%  0%  0%  0%\n",
      "[libx264 @ 0x55e10fd6cf00] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 11% 13% 65%  2%  2%  1%  2%  1%  2%\n",
      "[libx264 @ 0x55e10fd6cf00] i8c dc,h,v,p: 91%  6%  3%  0%\n",
      "[libx264 @ 0x55e10fd6cf00] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x55e10fd6cf00] ref P L0: 64.1%  1.7% 21.8% 12.4%\n",
      "[libx264 @ 0x55e10fd6cf00] ref B L0: 73.6% 22.1%  4.3%\n",
      "[libx264 @ 0x55e10fd6cf00] ref B L1: 94.2%  5.8%\n",
      "[libx264 @ 0x55e10fd6cf00] kb/s:65.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Pushing repo Naveen20o1/ppo-LunarLander-v2-firstagent to the Hugging\n",
      "Face Hub\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "policy.optimizer.pth:   0%|          | 0.00/88.4k [00:00<?, ?B/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "policy.optimizer.pth:  19%|‚ñà‚ñä        | 16.4k/88.4k [00:00<00:01, 50.7kB/s]\n",
      "\n",
      "\n",
      "\n",
      "policy.pth: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43.8k/43.8k [00:00<00:00, 62.0kB/s]\n",
      "policy.optimizer.pth: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88.4k/88.4k [00:00<00:00, 96.9kB/s]\n",
      "replay.mp4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 177k/177k [00:01<00:00, 168kB/s]  \n",
      "\n",
      "\u001b[A\n",
      "\n",
      "pytorch_variables.pth: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 864/864 [00:01<00:00, 498B/s]\n",
      "ppo-LunarLander-v2.zip: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148k/148k [00:01<00:00, 86.9kB/s] \n",
      "\n",
      "\n",
      "Upload 5 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:02<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m‚Ñπ Your model is pushed to the Hub. You can view your model here:\n",
      "https://huggingface.co/Naveen20o1/ppo-LunarLander-v2-firstagent/tree/main/\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Naveen20o1/ppo-LunarLander-v2-firstagent/commit/abf5333f57e2414ee9bb9e34eeb22a6e9e1450d3', commit_message='Upload our first PPO LunarLander-v2 trained agent', commit_description='', oid='abf5333f57e2414ee9bb9e34eeb22a6e9e1450d3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Naveen20o1/ppo-LunarLander-v2-firstagent', endpoint='https://huggingface.co', repo_type='model', repo_id='Naveen20o1/ppo-LunarLander-v2-firstagent'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization or UserName}/{repo_name} \n",
    "repo_id = f\"{USER_NAME}/ppo-LunarLander-v2-firstagent\" # Change with your repo id, you can't push with mine üòÑ\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload our first PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(model=model, # Our trained model\n",
    "               model_name=model_name, # The name of our trained model\n",
    "               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n",
    "               env_id=env_id, # Name of the environment\n",
    "               eval_env=eval_env, # Evaluation Environment\n",
    "               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "               commit_message=commit_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35 # 1 SMP Tue Nov 5 00:21:55 UTC 2024\n",
      "- Python: 3.9.21\n",
      "- Stable-Baselines3: 2.0.0a5\n",
      "- PyTorch: 2.6.0+cu124\n",
      "- GPU Enabled: True\n",
      "- Numpy: 2.0.2\n",
      "- Cloudpickle: 3.1.1\n",
      "- Gymnasium: 0.28.1\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35 # 1 SMP Tue Nov 5 00:21:55 UTC 2024\n",
      "- Python: 3.9.21\n",
      "- Stable-Baselines3: 2.0.0a5\n",
      "- PyTorch: 2.6.0+cu124\n",
      "- GPU Enabled: True\n",
      "- Numpy: 2.0.2\n",
      "- Cloudpickle: 3.1.1\n",
      "- Gymnasium: 0.28.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from huggingface_sb3 import load_from_hub\n",
    "checkpoint = load_from_hub(\n",
    "\trepo_id=\"Naveen20o1/ppo-LunarLander-v2-firstagent\",\n",
    "\tfilename=\"ppo-LunarLander-v2.zip\",\n",
    ")\n",
    "\n",
    "model = PPO.load(checkpoint, print_system_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=159.22 +/- 86.77216089024408\n"
     ]
    }
   ],
   "source": [
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
